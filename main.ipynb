{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aware-mattress"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
        "\n",
        "from tensorflow.keras.utils import Progbar\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms"
      ],
      "id": "aware-mattress",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "serious-answer"
      },
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ],
      "id": "serious-answer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pressing-decade"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()"
      ],
      "id": "pressing-decade",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEHjHfDBcGor"
      },
      "source": [
        "# plt.imshow(Image.open('/content/drive/MyDrive/train-2/ILSVRC2012_val_00005001.JPEG'))"
      ],
      "id": "KEHjHfDBcGor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "broadband-navigator"
      },
      "source": [
        "train_path = '/content/drive/MyDrive/train-2'\n",
        "val_path = '/content/drive/MyDrive/val'\n",
        "# print(val_path)\n",
        "train_img_paths = glob.glob(train_path + '/*.JPEG')\n",
        "val_img_paths = glob.glob(val_path + '/*.JPEG')\n"
      ],
      "id": "broadband-navigator",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "recorded-finding"
      },
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize = (10, 10))\n",
        "for ax, img_path in zip(axes.flatten(), train_img_paths) :\n",
        "    ax.imshow(Image.open(img_path))\n",
        "    ax.axis(\"off\")\n",
        "    ax.yaxis.set_visible(False), ax.xaxis.set_visible(False)"
      ],
      "id": "recorded-finding",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "better-copyright"
      },
      "source": [
        "BUFFER_SIZE = 400\n",
        "BATCH_SIZE = 1\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256"
      ],
      "id": "better-copyright",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFC6Bt9Osg_7"
      },
      "source": [
        "def preprocess(image):\n",
        "    with tf.name_scope('preprocess'):\n",
        "        # [0, 1] => [-1, 1]\n",
        "        return image * 2 - 1\n",
        "\n",
        "def deprocess(image):\n",
        "    with tf.name_scope('deprocess'):\n",
        "        # [-1, 1] => [0, 1]\n",
        "        return (image + 1) / 2\n",
        "\n",
        "def preprocess_lab(lab):\n",
        "    with tf.name_scope('preprocess_lab'):\n",
        "        L_chan, a_chan, b_chan = tf.unstack(lab, axis=2)\n",
        "        # L_chan: black and white with input range [0, 100]\n",
        "        # a_chan/b_chan: color channels with input range ~[-110, 110], not exact\n",
        "        # [0, 100] => [-1, 1],  ~[-110, 110] => [-1, 1]\n",
        "        return [L_chan / 50 - 1, a_chan / 110, b_chan / 110]\n",
        "\n",
        "def deprocess_lab(L_chan, a_chan, b_chan):\n",
        "    with tf.name_scope('deprocess_lab'):\n",
        "        #TODO This is axis=3 instead of axis=2 when deprocessing batch of images \n",
        "               # ( we process individual images but deprocess batches)\n",
        "        #return tf.stack([(L_chan + 1) / 2 * 100, a_chan * 110, b_chan * 110], axis=3)\n",
        "        return tf.stack([(L_chan + 1) / 2 * 100, a_chan * 110, b_chan * 110], axis=2)\n",
        "\n",
        "def augment(image, brightness):\n",
        "    # (a, b) color channels, combine with L channel and convert to rgb\n",
        "    a_chan, b_chan = tf.unstack(image, axis=3)\n",
        "    L_chan = tf.squeeze(brightness, axis=3)\n",
        "    lab = deprocess_lab(L_chan, a_chan, b_chan)\n",
        "    rgb = lab_to_rgb(lab)\n",
        "    return rgb\n",
        "\n",
        "def check_image(image):\n",
        "    assertion = tf.assert_equal(tf.shape(image)[-1], 3, message='image must have 3 color channels')\n",
        "    with tf.control_dependencies([assertion]):\n",
        "        image = tf.identity(image)\n",
        "\n",
        "    if image.get_shape().ndims not in (3, 4):\n",
        "        raise ValueError('image must be either 3 or 4 dimensions')\n",
        "\n",
        "    # make the last dimension 3 so that you can unstack the colors\n",
        "    shape = list(image.get_shape())\n",
        "    shape[-1] = 3\n",
        "    image.set_shape(shape)\n",
        "    return image\n",
        "\n",
        "def rgb_to_lab(srgb):\n",
        "    # based on https://github.com/torch/image/blob/9f65c30167b2048ecbe8b7befdc6b2d6d12baee9/generic/image.c\n",
        "    with tf.name_scope('rgb_to_lab'):\n",
        "        srgb = check_image(srgb)\n",
        "        srgb_pixels = tf.reshape(srgb, [-1, 3])\n",
        "        with tf.name_scope('srgb_to_xyz'):\n",
        "            linear_mask = tf.cast(srgb_pixels <= 0.04045, dtype=tf.float32)\n",
        "            exponential_mask = tf.cast(srgb_pixels > 0.04045, dtype=tf.float32)\n",
        "            rgb_pixels = (srgb_pixels / 12.92 * linear_mask) + (((srgb_pixels + 0.055) / 1.055) ** 2.4) * exponential_mask\n",
        "            rgb_to_xyz = tf.constant([\n",
        "                #    X        Y          Z\n",
        "                [0.412453, 0.212671, 0.019334], # R\n",
        "                [0.357580, 0.715160, 0.119193], # G\n",
        "                [0.180423, 0.072169, 0.950227], # B\n",
        "            ])\n",
        "            xyz_pixels = tf.matmul(rgb_pixels, rgb_to_xyz)\n",
        "\n",
        "        # https://en.wikipedia.org/wiki/Lab_color_space#CIELAB-CIEXYZ_conversions\n",
        "        with tf.name_scope('xyz_to_cielab'):\n",
        "            # convert to fx = f(X/Xn), fy = f(Y/Yn), fz = f(Z/Zn)\n",
        "\n",
        "            # normalize for D65 white point\n",
        "            xyz_normalized_pixels = tf.multiply(xyz_pixels, [1/0.950456, 1.0, 1/1.088754])\n",
        "\n",
        "            epsilon = 6/29\n",
        "            linear_mask = tf.cast(xyz_normalized_pixels <= (epsilon**3), dtype=tf.float32)\n",
        "            exponential_mask = tf.cast(xyz_normalized_pixels > (epsilon**3), dtype=tf.float32)\n",
        "            fxfyfz_pixels = (xyz_normalized_pixels / (3 * epsilon**2) + 4/29) * linear_mask + (xyz_normalized_pixels ** (1/3)) * exponential_mask\n",
        "\n",
        "            # convert to lab\n",
        "            fxfyfz_to_lab = tf.constant([\n",
        "                #  l       a       b\n",
        "                [  0.0,  500.0,    0.0], # fx\n",
        "                [116.0, -500.0,  200.0], # fy\n",
        "                [  0.0,    0.0, -200.0], # fz\n",
        "            ])\n",
        "            lab_pixels = tf.matmul(fxfyfz_pixels, fxfyfz_to_lab) + tf.constant([-16.0, 0.0, 0.0])\n",
        "\n",
        "        return tf.reshape(lab_pixels, tf.shape(srgb))\n",
        "\n",
        "\n",
        "def lab_to_rgb(lab):\n",
        "    with tf.name_scope('lab_to_rgb'):\n",
        "        lab = check_image(lab)\n",
        "        lab_pixels = tf.reshape(lab, [-1, 3])\n",
        "        # https://en.wikipedia.org/wiki/Lab_color_space#CIELAB-CIEXYZ_conversions\n",
        "        with tf.name_scope('cielab_to_xyz'):\n",
        "            # convert to fxfyfz\n",
        "            lab_to_fxfyfz = tf.constant([\n",
        "                #   fx      fy        fz\n",
        "                [1/116.0, 1/116.0,  1/116.0], # l\n",
        "                [1/500.0,     0.0,      0.0], # a\n",
        "                [    0.0,     0.0, -1/200.0], # b\n",
        "            ])\n",
        "            fxfyfz_pixels = tf.matmul(lab_pixels + tf.constant([16.0, 0.0, 0.0]), lab_to_fxfyfz)\n",
        "\n",
        "            # convert to xyz\n",
        "            epsilon = 6/29\n",
        "            linear_mask = tf.cast(fxfyfz_pixels <= epsilon, dtype=tf.float32)\n",
        "            exponential_mask = tf.cast(fxfyfz_pixels > epsilon, dtype=tf.float32)\n",
        "            xyz_pixels = (3 * epsilon**2 * (fxfyfz_pixels - 4/29)) * linear_mask + (fxfyfz_pixels ** 3) * exponential_mask\n",
        "\n",
        "            # denormalize for D65 white point\n",
        "            xyz_pixels = tf.multiply(xyz_pixels, [0.950456, 1.0, 1.088754])\n",
        "\n",
        "        with tf.name_scope('xyz_to_srgb'):\n",
        "            xyz_to_rgb = tf.constant([\n",
        "                #     r           g          b\n",
        "                [ 3.2404542, -0.9692660,  0.0556434], # x\n",
        "                [-1.5371385,  1.8760108, -0.2040259], # y\n",
        "                [-0.4985314,  0.0415560,  1.0572252], # z\n",
        "            ])\n",
        "            rgb_pixels = tf.matmul(xyz_pixels, xyz_to_rgb)\n",
        "            # avoid a slightly negative number messing up the conversion\n",
        "            rgb_pixels = tf.clip_by_value(rgb_pixels, 0.0, 1.0)\n",
        "            linear_mask = tf.cast(rgb_pixels <= 0.0031308, dtype=tf.float32)\n",
        "            exponential_mask = tf.cast(rgb_pixels > 0.0031308, dtype=tf.float32)\n",
        "            srgb_pixels = (rgb_pixels * 12.92 * linear_mask) + ((rgb_pixels ** (1/2.4) * 1.055) - 0.055) * exponential_mask\n",
        "\n",
        "        return tf.reshape(srgb_pixels, tf.shape(lab))"
      ],
      "id": "CFC6Bt9Osg_7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stretch-lawyer"
      },
      "source": [
        "def resize(image, height, width) :\n",
        "    reimage = tf.image.resize(image, [height, width], method = tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    return reimage\n",
        "\n",
        "def flip(input_image) :\n",
        "    image = input_image\n",
        "    if tf.random.uniform(()) > 0.5 :\n",
        "        image = tf.image.flip_left_right(input_image)\n",
        "    return image\n",
        "def load(img):\n",
        "    img = rgb2lab(img)\n",
        "    return img"
      ],
      "id": "stretch-lawyer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "velvet-undergraduate"
      },
      "source": [
        "def load_train_image(img_file) :\n",
        "    image = tf.io.read_file(img_file)\n",
        "    image = tf.io.decode_jpeg(image, channels = 3)\n",
        "    image = resize(image, IMG_HEIGHT, IMG_WIDTH)\n",
        "    image = flip(image)\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    \n",
        "    # lab = tf.py_function(func = load, inp = [image], Tout = tf.float32)\n",
        "    lab = rgb_to_lab(image)\n",
        "    L_chan, a_chan, b_chan = preprocess_lab(lab)\n",
        "    lab = deprocess_lab(L_chan, a_chan, b_chan)\n",
        "    # image = tf.cast(image, tf.uint8)\n",
        "    L_ = lab[:, :, 0:1]\n",
        "    ab_ = lab[:, :, 1:]\n",
        "\n",
        "    return L_, ab_\n",
        "\n",
        "def load_val_image(img_file) :\n",
        "    image = tf.io.read_file(img_file)\n",
        "    image = tf.io.decode_jpeg(image, channels = 3)\n",
        "    image = resize(image, IMG_HEIGHT, IMG_WIDTH)\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    # lab = tf.py_function(func = load, inp = [image], Tout = tf.float32)\n",
        "    lab = rgb_to_lab(image)\n",
        "    L_chan, a_chan, b_chan = preprocess_lab(lab)\n",
        "    lab = deprocess_lab(L_chan, a_chan, b_chan)\n",
        "    \n",
        "    # tf.cast(image, tf.uint8)\n",
        "    L_ = lab[:, :, 0:1]\n",
        "    ab_ = lab[:, :, 1:3]\n",
        "\n",
        "    return L_, ab_\n"
      ],
      "id": "velvet-undergraduate",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx2GOcblgtvg"
      },
      "source": [
        "train_images = []\n",
        "val_images = []\n",
        "for img_path in train_img_paths:\n",
        "    im = Image.open(img_path)\n",
        "    train_images.append(im)\n",
        "\n",
        "for val_path in val_img_paths:\n",
        "    im = Image.open(img_path)\n",
        "    val_images.append(im)\n",
        "\n",
        "\n",
        "def generate_dataset(images):\n",
        "    X = []\n",
        "    Y = []\n",
        "    for i in images:\n",
        "        i = resize(i, IMG_HEIGHT, IMG_WIDTH)\n",
        "        lab_img = lab2rgb(i / 255)\n",
        "        x = lab_img[:, :, 0]\n",
        "        y = lab_img[:, :, 1:]\n",
        "        X.append(x.reshape(256, 256, 1))\n",
        "        Y.append(y)\n",
        "    X = np.array(x, dtype = np.float32)\n",
        "    Y = np.array(y, dtype = np.float32)\n",
        "    return X, Y\n",
        "\n",
        "X_train, Y_train = generate_images(train_images)\n",
        "X_test, Y_test = generate_images(val_images)\n"
      ],
      "id": "bx2GOcblgtvg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "italic-primary"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)"
      ],
      "id": "italic-primary",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "executed-practitioner"
      },
      "source": [
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE)"
      ],
      "id": "executed-practitioner",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frequent-adapter"
      },
      "source": [
        "OUTPUT_CHANELS = 2"
      ],
      "id": "frequent-adapter",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "steady-keeping"
      },
      "source": [
        "**UNeT**"
      ],
      "id": "steady-keeping"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "patent-fraction"
      },
      "source": [
        "def downsample(filter, size, apply_batchnorm = True):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(keras.layers.Conv2D(filter, size, padding = 'same',\n",
        "                                   strides = 2, kernel_initializer = initializer,\n",
        "                                   use_bias = False))\n",
        "    if apply_batchnorm :\n",
        "        result.add(keras.layers.BatchNormalization())\n",
        "    result.add(keras.layers.Dropout(rate = 0.5))\n",
        "    result.add(keras.layers.LeakyReLU())\n",
        "    return result"
      ],
      "id": "patent-fraction",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "narrow-vintage"
      },
      "source": [
        "def upsample(filter, size, apply_dropout = False):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(keras.layers.Conv2DTranspose(filter, size, padding = 'same',\n",
        "                                            strides = 2, kernel_initializer= initializer,\n",
        "                                            use_bias = False))\n",
        "    result.add(keras.layers.BatchNormalization())\n",
        "    if apply_dropout :\n",
        "        result.add(keras.layers.Dropout(rate = 0.5))\n",
        "    result.add(keras.layers.ReLU())\n",
        "    return result"
      ],
      "id": "narrow-vintage",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "genuine-partition"
      },
      "source": [
        "def Generator():\n",
        "    inputs = keras.layers.Input(shape = [IMG_HEIGHT, IMG_WIDTH, 1])\n",
        "    \n",
        "    #bs == batch_size\n",
        "    \n",
        "    encoder = [\n",
        "        downsample(64, 4, False), #[bs, 128, 128, 64]\n",
        "        downsample(128, 4),       #[bs, 64, 64, 128]    \n",
        "        downsample(256, 4),       #[bs, 32, 32, 256]\n",
        "        downsample(512, 4),       #[bs, 16, 16, 512] \n",
        "        downsample(512, 4),       #[bs, 8, 8, 512]\n",
        "        downsample(512, 4),       #[bs, 4, 4, 512]    \n",
        "        downsample(512, 4),       #[bs, 2, 2, 512]\n",
        "        downsample(512, 4)         #[bs, 1, 1, 512]\n",
        "    ]\n",
        "    \n",
        "    decoder = [\n",
        "        upsample(512, 4, True), #[bs, 2, 2, 512 * 2]\n",
        "        upsample(512, 4, True), #[bs, 4, 4, 512 * 2]\n",
        "        upsample(512, 4, True), #[bs, 8, 8, 512 * 2]\n",
        "        upsample(512, 4),        #[bs, 16, 16, 512 * 2]    \n",
        "        upsample(256, 4),        #[bs, 32, 32, 256 * 2] \n",
        "        upsample(128, 4),        #[bs, 64, 64, 128 * 2]\n",
        "        upsample(64, 4)          #[bs, 128, 128, 64 * 2]  \n",
        "    ]\n",
        "    \n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    \n",
        "    last = keras.layers.Conv2DTranspose(OUTPUT_CHANELS, 4, \n",
        "                                        strides = 2, padding = 'same', \n",
        "                                        kernel_initializer = initializer, activation = 'tanh' )\n",
        "    \n",
        "    x = inputs\n",
        "    skips = []\n",
        "    for down in encoder :\n",
        "        x = down(x)\n",
        "        skips.append(x)\n",
        "    \n",
        "    skips = reversed(skips[:-1])\n",
        "    \n",
        "    for up, skip in zip(decoder, skips) :\n",
        "        x = up(x)\n",
        "        x = keras.layers.Concatenate()([x, skip])\n",
        "        \n",
        "    output = last(x)\n",
        "    \n",
        "    return keras.Model(inputs = inputs, outputs = output) "
      ],
      "id": "genuine-partition",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ranking-perception"
      },
      "source": [
        "generator = Generator()\n",
        "# keras.utils.plot_model(generator, show_shapes = True, dpi = 64)"
      ],
      "id": "ranking-perception",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "immediate-coordinator"
      },
      "source": [
        "LAMDA = 100\n",
        "BCE_loss = keras.losses.BinaryCrossentropy(from_logits = True)"
      ],
      "id": "immediate-coordinator",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "angry-completion"
      },
      "source": [
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "    gan_loss = BCE_loss(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "    l1_loss = keras.losses.MAE(target, gen_output)\n",
        "    net_loss = gan_loss + l1_loss\n",
        "    return net_loss, gan_loss, l1_loss"
      ],
      "id": "angry-completion",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "collected-station"
      },
      "source": [
        "def Discriminator() :\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    \n",
        "    inp = keras.layers.Input(shape = [IMG_HEIGHT, IMG_WIDTH, 1], name = 'input_image')\n",
        "    tar = keras.layers.Input(shape = [IMG_HEIGHT, IMG_WIDTH, 3], name = 'target_image')\n",
        "    \n",
        "    x  = keras.layers.concatenate([inp, tar])\n",
        "    x_1 = downsample(64, 4, False)(x)\n",
        "    x_2 = downsample(128, 4)(x_1)\n",
        "    x_3 = downsample(256, 4)(x_2)\n",
        "    \n",
        "    zero_pad_1 = keras.layers.ZeroPadding2D()(x_3)\n",
        "    conv_1 = keras.layers.Conv2D(512, 4, kernel_initializer = initializer, use_bias = False)(zero_pad_1)\n",
        "    batchnorm_1 = keras.layers.BatchNormalization()(conv_1)\n",
        "    activation_1 = keras.layers.LeakyReLU()(batchnorm_1)\n",
        "    zero_pad_2 = keras.layers.ZeroPadding2D()(activation_1)\n",
        "    \n",
        "    last = keras.layers.Conv2D(1, 4, kernel_initializer = initializer)(zero_pad_2)\n",
        "    \n",
        "    return keras.Model(inputs = [inp, tar], outputs = last)   "
      ],
      "id": "collected-station",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "modern-event"
      },
      "source": [
        "discriminator = Discriminator()"
      ],
      "id": "modern-event",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "little-pressing"
      },
      "source": [
        "def discriminator_loss(disc_generated_output, disc_real_output):\n",
        "    real_loss = BCE_loss(tf.ones_like(disc_real_output), disc_real_output)\n",
        "    gen_loss = BCE_loss(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "    net_loss = gen_loss + real_loss\n",
        "    return net_loss"
      ],
      "id": "little-pressing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "theoretical-palace"
      },
      "source": [
        "generator_optimizer = keras.optimizers.Adam(2e-4, beta_1 = 0.5)\n",
        "discriminator_optimizer = keras.optimizers.Adam(2e-4, beta_1 = 0.5)"
      ],
      "id": "theoretical-palace",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "controversial-asbestos"
      },
      "source": [
        "# Training Checkpoint\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer = generator_optimizer, \n",
        "                                 discriminator_optimizer = discriminator_optimizer, \n",
        "                                 generator = generator, discriminator = discriminator\n",
        "                                )\n",
        "\n"
      ],
      "id": "controversial-asbestos",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "large-commission"
      },
      "source": [
        "EPOCHS = 5"
      ],
      "id": "large-commission",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "collaborative-cache"
      },
      "source": [
        "import datetime\n",
        "log_dir=\"logs/\"\n",
        "\n",
        "summary_writer = tf.summary.create_file_writer(\n",
        "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
      ],
      "id": "collaborative-cache",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "first-england"
      },
      "source": [
        "@tf.function\n",
        "\n",
        "def train_step(input_image, target, epoch) :\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape :\n",
        "        gen_output = generator(input_image, training = True)\n",
        "        \n",
        "        fake_image = tf.concat([input_image, gen_output], axis = -1)\n",
        "        real_image = tf.concat([input_image, target], axis = -1)\n",
        "        \n",
        "        disc_real_output = discriminator([input_image, real_image], training = True)\n",
        "        disc_generated_output = discriminator([input_image, fake_image], training = True)\n",
        "        \n",
        "        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
        "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "        \n",
        "    generator_grads = gen_tape.gradient(gen_total_loss, generator.trainable_variables)\n",
        "    discriminator_grads = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "        \n",
        "    generator_optimizer.apply_gradients(zip(generator_grads, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(discriminator_grads, discriminator.trainable_variables))\n",
        "        \n",
        "\n",
        "    \n",
        "    return gen_total_loss, gen_gan_loss, gen_l1_loss, disc_loss"
      ],
      "id": "first-england",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "partial-warrior"
      },
      "source": [
        "def fit(train_ds, val_ds, epochs):\n",
        "    for epoch in range(epochs) :\n",
        "        print(\"\\nEpoch {}/{}\".format(epoch + 1,epochs))\n",
        "        start = time.time()\n",
        "        \n",
        "        num_training_samples = len(train_img_paths)\n",
        "\n",
        "        pb_i = Progbar(num_training_samples)\n",
        "        \n",
        "        ii = 0\n",
        "        for n, (train_input, train_target) in train_ds.enumerate():\n",
        "            gen_total_loss, gen_gan_loss, gen_l1_loss, disc_loss = train_step(train_input, train_target, epoch)\n",
        "            vals = [('gen_loss', gen_total_loss), ('gen_gan_loss', gen_gan_loss), ('gen_l1_loss', gen_l1_loss), ('disc_loss', disc_loss)]\n",
        "            pb_i.update(ii * BATCH_SIZE, values = vals)\n",
        "            ii = ii + 1 \n",
        "\n",
        "    if (epoch + 1) % 20 == 0 :\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "        print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
        "                                                        time.time()-start))\n",
        "    checkpoint.save(file_prefix=checkpoint_prefix)"
      ],
      "id": "partial-warrior",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "incorporate-gambling"
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "id": "incorporate-gambling",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "higher-dragon"
      },
      "source": [
        "with tf.device(device_name) :\n",
        "    fit(train_dataset, val_dataset, EPOCHS)"
      ],
      "id": "higher-dragon",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCdyaPIu0gNy"
      },
      "source": [
        "Y_hat = generator(X_test[:25])\n",
        "total_count = len(Y_hat)\n",
        "\n",
        "for idx, (x, y, y_hat) in enumerate(zip(X_test[:25], Y_test[:25], Y_hat)):\n",
        "\n",
        "    # Original RGB image\n",
        "    orig_lab = np.dstack((x, y * 128))\n",
        "    orig_rgb = lab2rgb(orig_lab)\n",
        "\n",
        "    # Grayscale version of the original image\n",
        "    grayscale_lab = np.dstack((x, np.zeros((IMAGE_SIZE, IMAGE_SIZE, 2))))\n",
        "    grayscale_rgb = lab2rgb(grayscale_lab)\n",
        "\n",
        "    # Colorized image\n",
        "    predicted_lab = np.dstack((x, y_hat * 128))\n",
        "    predicted_rgb = lab2rgb(predicted_lab)\n",
        "\n",
        "\n",
        "    \n",
        "    plt.axis('off')\n",
        "    plt.imshow(grayscale_rgb)\n",
        "    # plt.savefig(os.path.join(WORKDIR, 'results', '{}-bw.png'.format(idx)), dpi=1)\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.imshow(orig_rgb)\n",
        "    # plt.savefig(os.path.join(WORKDIR, 'results', '{}-gt.png'.format(idx)), dpi=1)\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.imshow(predicted_rgb)\n",
        "    # plt.savefig(os.path.join(WORKDIR, 'results', '{}-gan.png'.format(idx)), dpi=1)\n",
        "\n",
        "    sys.stdout.flush()\n",
        "    # sys.stdout.write('\\r{} / {}'.format(idx + 1, total_count))"
      ],
      "id": "BCdyaPIu0gNy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL_0e08KMyJ3"
      },
      "source": [
        ""
      ],
      "id": "gL_0e08KMyJ3",
      "execution_count": null,
      "outputs": []
    }
  ]
}